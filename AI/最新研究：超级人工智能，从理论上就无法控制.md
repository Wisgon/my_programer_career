> 计算的能力是有极限的，但因此，人类无法控制超级人工智能。



![图片](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibhMApj9QPMRAol6LMGxmuOic7977MWOiaGPicwibKgXCVuc6wuCHSj5gbxX4m7Q0e4FaLqiaMXtGKEAHg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



近日，一项新的研究发现，从理论上来看，人类不可能控制超级人工智能。更为糟糕的是，这项研究也明确了人类无法在这种 AI 生成之时发现它。



让人稍稍感到安慰的是，根据不止一项预测，任何通用超级计算机超越人类的时间都会在数十年以后。



最近几年，人工智能在国际象棋、围棋、德州扑克、Jeopardy 等游戏上超越了人类，在 Dota2、星际争霸游戏中和顶级玩家打得有来有回，时不时会引起一小阵恐慌，有人担心超越人类的机器智能会在某一天让人们无所适从。「有关超级智能是否接受人类控制的问题其实很古老，」西班牙马德里自治大学计算机科学家 Manuel Alfonseca 说道，「这还得追溯到 20 世纪 40 年代的阿西莫夫的机器人三定律。」



![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

*与超级人工智能展开任何形式的交流都是有风险的。*



人们耳熟能详的机器人三定律，首先在科幻小说家伊萨克 · 阿西莫夫在 1942 年的短篇小说集《我，机器人》中被提出，规则如下：



1. 机器人不得伤害人类，或坐视人类受到伤害；
2. 机器人必须服从人类命令，除非命令与第一法则发生冲突；
3. 在不违背第一或第二法则之下，机器人可以保护自己。



在 1985 年《机器人与帝国》书中，阿西莫夫将三大法则扩张为四大法则：加入第零法则——机器人不得伤害整体人类，或坐视整体人类受到伤害。



2014 年，牛津大学人类未来研究所（Future of Humanity Institute ）主任、哲学家 Nick Bostrom 不仅探索了超级人工智能破坏人类的方式，还研究了针对这类机器可能的控制策略以及它们无法奏效的原因。



![图片](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibhMApj9QPMRAol6LMGxmuOaEZlZOFv1LUjaKARr3g3r7gnVOMVeAyoL8ictPTgbviaqPIlklAIGMrg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*Nick Bostrom。*



Bostrom 列举了这一「控制问题」的两种可能解决方案。一种是控制 AI 能做的事情，如阻止 AI 连接互联网；另一种是控制 AI 想做的事情，如教授 AI 规则和价值观，使其秉持人类利益最大的原则行动。Bostrom 认为，第一种解决方案存在的问题是超级智能机器可能挣脱任何人类施加给它的限制；第二种解决方案则担心人类可能没有能力训练出超级人工智能。



**计算固有的限制可能导致人类无法控制超级人工智能**



在本月初发表在 AI 领域顶级期刊《人工智能研究杂志》（JAIR）上的一篇文章中，马德里自治大学、马克斯 - 普朗克人类发展研究所等机构的研究者认为，**由于计算本身固有的基本限制，人类可能无法控制超级人工智能**。



他们表示，任何旨在确保超级人工智能无法伤害人类的算法都必须首先模拟机器行为以预测其行动的潜在后果。如果超级智能机器确实可能造成伤害，那么此类抑制算法（containment algorithm）需要停止机器的运行。



然而，科学家们认为，任何抑制算法都不可能模拟 AI 的行为，也就无法百分之百地预测 AI 的行为是否会造成伤害。抑制算法可能无法正确模拟 AI 的行为或准确预测 AI 行动的后果，也就无法分辨出这些失败。



![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)



论文地址：https://jair.org/index.php/jair/article/view/12202/26642



「阿西莫夫给出的第一定律实际上已被证明是无法计算的，」Alfonseca 说道，「因而根本无法实现。」



我们甚至连自己是否已经创造了超级智能机器都不知道——这是可计算理论中莱斯定理的一个推论，该定理指出递归可枚举语言的所有非平凡（nontrival）性质都是不可判定的。从本质上来说，我们不能仅通过观察程序本身，就知道程序可能会输出什么。



当然另一方面，我们还不需要为将来的机器人主宰提前做好服侍的准备。在该研究中，有三个重要问题给该论文的推论带来了不确定性。



- 首先，Alfonseca 预计强人工智能的关键时刻还很遥远，他说道：「至少还有两个世纪。」
- 其次，人们所说的通用人工智能，或强人工智能在理论上是否可行，其实也是未知数。「这是指能像人类一样能够处理多个领域任务的机器。」
- 最后，Alfonseca 说道：「我们还没有证明超级人工智能永远无法控制，我只是说它们不能被永远控制。」





![图片](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibhMApj9QPMRAol6LMGxmuOhNE0J99uUKdgjXSYKbZme4AKCZorWSQjxnRuSldBWD9Iw35jr32Aicw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

*计算复杂性是阻止人类控制强人工智能的重要原因。*



尽管我们可能无法控制一个强人工智能体，但控制一个超越人类水平的狭义 AI 是可行的——我们可以放心地依赖一些专注于某些功能，而非像人类一样可以执行多种任务的机器人。「我们已经拥有这种超级智能，」Alfonseca 说道。「例如我们拥有可以比人类速度快很多的计算机器。这也是一种狭义的超级智能。」



*原文链接：https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/super-artificialintelligence*